<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>NLP Introduction Overview</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/serif-nlp.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			let link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown>
					<script type="text/template">
						# NLP Workshop
						## Part 2: Deep NLP

						[olaf.janssen@fontys.nl](olaf.janssen@fontys.nl)

						[Write with GPT-2](https://transformer.huggingface.co/doc/distil-gpt2)

						[GPT-3 on askReddit](https://www.kmeme.com/2020/10/gpt-3-bot-went-undetected-askreddit-for.html)

						[https://olafjanssen.github.io/nlp-workshop/](https://olafjanssen.github.io/nlp-workshop/)
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## **Generate** new text
						
						From Vector-to-Vector to Sequence-to-Sequence

						![](https://karpathy.github.io/assets/rnn/diags.jpeg)

					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Seq2Seq Timeline

						- 1986 - Recurrent Neural Networks (RNN)
						- 1997 - Long Short-Term Memory (LSTM)
						- Encoder-Decoder architecture
						- 2017 - Transformers (robots in disguise)

						[TLDR;](https://www.youtube.com/watch?v=TQQlZhbC5ps&ab_channel=CodeEmporium)
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Recurrent Neural Networks (RNN)

						Tries to tackle that A/CNNs expect fixed input and output length, by allowing to rembember previous results, see [explanation](https://victorzhou.com/blog/intro-to-rnns/) and [implementation](https://towardsdatascience.com/implementation-of-rnn-lstm-and-gru-a4250bf6c090).
						
						**Pro:** simple, understandable implementation, useful for simple autocomplete

						**Cons:** exploding/vanishing gradients makes it impractical for larger problems, training is sequential and thus slow
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Long-Short-Term Memory (LSTM)

						To improve on RNNs, we can use a **[Long Short-Term Memory (LSTM)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)** layer ([more explanation](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)), by adding more elaborate forget/memory gates.

						With an LSTM you also capture the time dimension in the [memory cell](https://livebook.manning.com/book/natural-language-processing-in-action/chapter-9/31) of the LSTM layer: **thought vector**. We call this state the thought vector. It not only summarizes a text, it encodes the train-of-thought and context of a statement.

						**Pros:** has longer 'memory' than RNN

						**Cons:** still slow (sequential input). You have little control on the content. Need lots of data for correct grammar.
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## **Generate** new text with the Encoder-Decoder architecture (and LSTMs)

						To improve reuse and text generation we split the problem into an [encoder and decoder](https://livebook.manning.com/book/natural-language-processing-in-action/chapter-10/22) ([more explanation](https://medium.com/analytics-vidhya/intuitive-understanding-of-seq2seq-model-attention-mechanism-in-deep-learning-1c1c24aace1e)).

						Applications:
						- [machine translations](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)
						- Chatbot conversations, Q&A bots, Document summarization, Produce new works of Shakespeare
						- Co-authoring books ([Ronald Giphart's _Ik, Robot_](https://www.vice.com/nl/article/bj7ve4/we-spraken-ronald-giphart-over-hoe-hij-zijn-nieuwe-werk-samen-met-ai-maakte))
						- [Reproducing Trump tweets](https://github.com/eleurent/make-lstm-great-again)

						**Cons:** the thought vector is still very limited, so that e.g. grammar is not properly picked up and relations between words between sentences
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Transformers

						Since the introduction of the Bi-directional [Transformer architecture](https://jalammar.github.io/illustrated-transformer/)[2](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04). NLP has taken another leap. Meaning is no longer context-free as with word embeddings but completely depended on context using **attention**, notable examples: BERT, GPT-2/3, XLNET, T5.
						
						Transformer based models use _self-attention_ and _positional encoding_, _segment encoding_, _multi-headed attention_... 
						
						**Pros:** allows transfer learning, limit has not been reached yet, applicable to other domains

						- [Explanation of BERT](https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/)
						- [BERT Demo](http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)
						- [BERT visualisations](https://github.com/jessevig/bertviz)
						- [Illustrated GPT-2 explanation](https://jalammar.github.io/illustrated-gpt2/)
						- [Transfer learning with Transformers](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Beyond the model

						- How to prevent text-loops, run-off sentences, bad grammar? With [beam-search](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24), [visualized](https://seq2seq-vis.io/).
						- How to validate your output? [Bleu Score / Word Error Metric](https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b)
						
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## X. From text to knowledge

						**Entity** and relation extraction:
						- "Lecturers works at Fontys", "Olaf is a lecturer"
							- 'Olaf' → name person, 'Fontys' → name school, 'lecturer' → occupation
							- ['Olaf', 'is-a', 'lecturer']
							- ['lecturer', 'works-for', 'Fontys']

						The resulting **knowledge graph** can be [queried](https://quepy.readthedocs.io/en/latest/index.html):
							- "List work-for Fontys"
							- "What is Olaf?"
							- "Olaf works-for Fontys?"

						**Cons:** Knowledge graphs are slowly crawling out of a depressing winter, so examples are often stale
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Is it a model or is it an agent?

						Independent algorithms can statistically classify, summarize meaning and find content related in meaning, predict and generate new text.

						**But** while it appears to understand text, does it understand its own output? Watch an [interview with GPT-3](https://www.youtube.com/watch?v=PqbB07n_uQ4&ab_channel=EricElliott).
						
						When is 100% of what we read generated text? 
						When does it become conscious?

						What would it take to make the next leap?

						- combine with other modalities?
						- fact checking, world building?
						- spell/grammar check its own output?
						- do sentiment analysis of its own output and choose a proper tone of voice?
						- reject answers that are in meaning too far away from the user's input?
						- ...
					</script>
				</section>
			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true }
				]
			});
		</script>
	</body>
</html>
