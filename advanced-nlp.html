<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>NLP Introduction Overview</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/serif-nlp.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			let link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown>
					<script type="text/template">
						# NLP Workshop
						## Part 2: Advanced NLP

						[olaf.janssen@fontys.nl](olaf.janssen@fontys.nl)

					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### Content
						
						- Transformers (X-formers) and Attention mechanism
						- X-formers family tree
						- A bit about prompting and beyond...
					</script>
				</section>


				<section data-markdown>
					<script type="text/template">
						### Main sources for this workshop
						- [Generative NLP recap and non-technical explainer](https://ig.ft.com/generative-ai/)
						- [Illustrated Transformer explanation](https://jalammar.github.io/illustrated-transformer/)
						- [HuggingFace x-former overview](https://huggingface.co/transformers/v4.5.1/model_summary.html)
						- [X-former model catalogue overview](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/)
						- [Reinforcement Learning from Human Feedback](https://huggingface.co/blog/rlhf)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### Short historical and conceptual timeline

![Image](https://i0.wp.com/www.searchenginepeople.com/wp-content/uploads/2010/10/clip_image032.jpg?ssl=1)

(1986) - Autocomplete (RNN), (1997) - Machine Translation (LSTM), (2017) - Text generation (Transformers)

**SCALE UP (125M -> 173B parameters)**

* ... still glorified autocomplete?
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						### A parallelizable Seq2Seq using Attention
						![Image](https://jalammar.github.io/images/t/the_transformer_3.png)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### The original image
						![Image](https://miro.medium.com/v2/resize:fit:640/format:webp/1*3pxDWM3c1R_WSW7hVKoaRA.png)
					</script>
				</section>


				<section data-markdown>
					<script type="text/template">
						### Encoder - Decoder architecture
						![Image](https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### Embedding and self-attention
						![Image](https://jalammar.github.io/images/t/encoder_with_tensors_2.png)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### Query retrieval system analogue

						Query ~ search engine query<br>
						Key ~ title of a web page<br>
						Value ~ content of a web page<br>
						
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### QKV for X-transformers

						- Query (Q): Represents the information we're currently looking at.<br>
						- Key (K): Represents all the information we <b>can</b> attend to.<br>
						- Value (V): Contains the actual content of the information we want to focus on.<br>
								
						Imagine you're at a party and you (the Query) are trying to find your friend Bob (the Key). Once you find Bob, you listen to what he's saying (the Value).
						
						The attention mechanism calculates a score by comparing the Query with each Key to determine how much attention should be paid to the corresponding Value.

					</script>
				</section>





				<section data-markdown>
					<script type="text/template">
						### Using Q-K-V
						![Image](https://jalammar.github.io/images/t/transformer_self_attention_vectors.png)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### Using Q-K-V
						![Image](https://jalammar.github.io/images/t/self-attention-output.png)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### Multi-headed attention
						We can split the embedding in (logically) random pieces.
						![Image](https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### Resulting attention
						![Image](https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### Decoder - Step 1
						Create a new Key / Value from the last encoder.
						
						![Image](https://jalammar.github.io/images/t/transformer_decoding_1.gif)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### Decoder - Next steps 
						Feed-forward the Query through the decoder.

						![Image](https://jalammar.github.io/images/t/transformer_decoding_2.gif)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### X-former family tree
						![Image](https://amatriain.net/blog/images/02-05.png)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### X-former timeline 1
						![Image](https://amatriain.net/blog/images/02-06.png)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### X-former timeline 2
						![Image](https://amatriain.net/blog/images/02-09.png)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### Base vs fine-tuned
						![Image](https://python.langchain.com/assets/images/OSS_LLM_overview-b0a96cc35216ec43c3ccde7ed1140854.png)
					</script>
				</section>

				

				<section data-markdown>
					<script type="text/template">
						### Short good-to-know
						* Decoder only: GPTs (text completion)
							- special shoutout to Llama / [Alpaca](https://github.com/antimatter15/alpaca.cpp)
						* Encoder only: BERTs (classification)
							- special shoutout to DistilBERT
						* Encoder-Decoder: T5 (translation / task specific)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### The full table
						
						[Full catalogue of x-former descriptions](https://docs.google.com/spreadsheets/d/1ltyrAB6BL29cOv2fSpNQnnq2vbX8UrHl47d7FkIf6t4/edit#gid=0)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### Prompt engineering

						Remember: GTP-3 does not mimic a single human author, but resembles a **superposition of authors**.
						
						- We therefore have to take a **subtractive approach** to prompt engineering.
							- Define in human language *which* author should respond.
							- Restrict the answer space.  
				</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						### What does scale/tuning do in how you use a language model?

- Many-shot (T5)
- Few-shot (GPT2)
- One-shot (GPT3)
- Zero-shot prompts (instructGPT)
- More-than-I-asked-for (chatGPT)

					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						### Prompt strategies

- By demonstration (N-shot)
- Direct:<br> - `Translate French to English`
- By Proxy<br> - `Answer this question as Mahatma Gandhi`
- In natural context<br> - `Life is good, or as they say in Italy:`
- Constraining answer space and syntax:<br> - `Translate to French. English: "Hello!" French: "`

In general: setting the the same location as starting position in context-space.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### Metaprompt engineering
LLMs perform poorly on complex closed questions,  
but it often knows *how* and *if* it can solve those problems.

What are we missing?

We need to **extend the window of deliberation**: 
- perform silent reasoning
- avoid premature verdicts
- add moments of self-reflection
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						### Metaprompt strategies


Serialize reasoning and prevent rationalization.

- Use *metacognition*: 
	- `"Let's solve this problem by splitting it into steps."`
- Test the probabilty of the continuation: 
	- `"Thus, the correct answer is:"`
- Expert generator: 
	- `I create an expert generator that picks an expert most qualified to answer this question.`
- Self-curation and self-evaluation:  
	- `"Check the answer to the original question."`
					</script>
				</section>
			
				<section data-markdown>
					<script type="text/template">
						### What's going on now?
						
						- [Reinforcement Learning from Human Feedback (human alignment)](https://huggingface.co/blog/rlhf)
						- Multi-modality
						- External connections, Plug-in integration, Tool usage
						
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### So....
						- [GPT4all run local models testbed](https://gpt4all.io/)
						- [AutoGPT agents](https://auto-gpt.ai/)
						- [AgentGPT agents](https://agentgpt.reworkd.ai/)
						- [LangChain framework](https://python.langchain.com/docs/get_started/introduction)
						- [LangChain explainer video](https://youtu.be/aywZrzNaKjs?si=DZM1Y3ORchFMHh7X)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### LangChain overview
						![Image](https://daxg39y63pxwu.cloudfront.net/images/blog/langchain/LangChain.webp)
						
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### LangChain overview
						![Image](https://community.intersystems.com/sites/default/files/inline/images/images/image(6597).png)
						
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						### StreamLit
						![Image](https://global.discourse-cdn.com/business7/uploads/streamlit/original/3X/4/a/4af3711af2abe5b9d4eae4ee914951884825d14c.jpeg)
						
					</script>
				</section>
	
				<section data-markdown>
					<script type="text/template">
						### So....

						Stop or not?
					</script>
				</section>
			
			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true }
				]
			});
		</script>
	</body>
</html>
