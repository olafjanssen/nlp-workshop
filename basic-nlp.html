<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>NLP Introduction Overview</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/serif-nlp.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			let link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown>
					<script type="text/template">
						# NLP Workshop

						## Part 1: NLP Basics

						But first:
						[AI Dungeon Game](https://play.aidungeon.io/)

						[olaf.janssen@fontys.nl](olaf.janssen@fontys.nl)

						[https://olafjanssen.github.io/nlp-workshop/](https://olafjanssen.github.io/nlp-workshop/)
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## This workshop

						1. NLP introduction
						2. Traditional NLP Pipeline
						3. Transformer architecture

					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Why is NLP challenging?
						(compared to programming languages, or other methods of communication)

						- no strict rules and protocols
						- lousy information density
						- text is ambiguous
						- most utterances are not instructions or facts
						- even humans are often quite crappy at NLP
						- often depends on a life-time of learned common knowledge for context
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Why is NLP so promising/interesting?
						
						- convert concepts of any complexity into a linear chain of words, which can be understood by humans
						- once a model understands language, we can tell it do whatever we want

					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Examples

						- machine translators
						- sentiment analysers
						- text summarizers
						- entity extraction (names, locations, events, ...)
						- chatbots
						- image search/captioning
						- text classification (spam, fake news, pro or con)
						- speech recognition
						- search engines

						Check out the list at [sebastianruder/NLP-progress](https://github.com/sebastianruder/NLP-progress).
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Notable resources

						* [HuggingFace](https://huggingface.co/) - a hub for NLP models
						* [LangChain](https://python.langchain.com/) - a framework for creating agents
						* [StreamLit](https://streamlit.io/) - deploying your NLP app

					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## NLP Pipeline

						1. **tokenize**: break up your text data into chunks
						2. turn text into **numerical data**
						3. condense the data so it captures **meaning**
						4. **search** for a response using the meaning
						5. use encoded response to **generate** new text

						(Not all applications implement all steps.)
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 1. Breaking up your text

						Use: [spaCy](https://spacy.io) or [NLTK](https://www.nltk.org)
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 1. Breaking up your text

						**corpus**: total collection of **documents** of text you use
						* e-mail collection, news articles, web pages, chat conversations
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 1. Breaking up your text

						**segments**: cut document into meaningful units (sentences)
						* 2021 is a way to start a sentence, i.e. that stinks?!
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 1. Breaking up your text

						* **tokens**: smallest elements you work with (usually words)
							+ `"I am red." → ["I", "am", "red"]`
							+ `"don’t" → [“do”, “n’t”] or [“don’t”] or [“do”, “not”]`
						* consider **n-grams**
							+ `["ice cream"]` (bi-gram)
							+ `["trick or treat"]` (tri-gram)

						[Huggingface Tokenizers](https://huggingface.co/docs/tokenizers/index)
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 1. Breaking up your text

						build a **lexicon** (your 'dictionary/vocabulary')
						* contains tokens + n-grams - stop words

					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 1. Breaking up your text

						additional (optional) cleaning of your lexicon:

						* **normalization**
							+ `["trump", "Trump", "TRUMP"] → "Trump"`
						* **stemming** (pre-, post-fixes)
							+ `["clothes", "cloth", "clothing"] → "cloth"`
						* **lemmatization** (combine similar meaning)
							+ `["do", "did", "done"] → ["do"]`
							+ `["lots", "plenty", "heaps", "tons", "numerous"] → ["many"]`

					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 1. Breaking up your text

						One last thing you may encounter:

						"You **(SUBJECT/PRONOUN)** eat **(VERB)** large **(OBJECT/ADJECTIVE)** apples **(OBJECT/NOUN)**."

						Part of Speech tagging, *don't try it at home*.
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 1. Breaking up your text
						
						Some tutorials:
						- [Tutorial using spaCy](https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/)
						- [Tutorial using NLTK](https://data-flair.training/blogs/nltk-python-tutorial/)

						Nowadays, you will probably no longer need to do this.
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 2. Turn text into **numerical data**
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 2. Turn text into **numerical data**

						We can use statistics and basic machine learning once we turn words into numbers.

						The simplest is: **one-hot encoding**.

						* Dog → [1, 0, 0, ....., 0]
						* Cat → [0, 1, 0, ....., 0]
						* Zorblax → [0, 0, 0, ....., 1]

						**Cons:** huge (empty) matrices, contains little statistical information
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 2. Turn text into **numerical data**

						**[Bag of Words (BOW)](https://pythonprogramminglanguage.com/bag-of-words/)**, or count dictionary

						* “A duck is a bird” → [“duck”: 1, “is”: 1, “a”: 2, “bird”: 1]

						**Pros:** summarizes an entire segment statistically, <br>classifier can be trained with a single long vector instead of sparse matrix

						**Cons:** you lose word order
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 2. Turn text into **numerical data**

						When comparing data for comparable content, normalize on sentence length.

						* “A duck is a bird” → [“duck”: 0.2, “is”: 0.2, “a”: 0.4, “bird”: 0.2]

						Or boost unique words with a **TF-IDF (Term Frequency - Inverse Document Frequency)** vector.

						**Pros:** summarizes an entire segment statistically compared to the rest of the corpus
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 2. Turn text into **numerical data**

						Comparing data using distance metrics to find closest (statistical) matches:

						* **Euclidian distance** (Pythagorean distance along all dimensions)
						* **[cosine similarity](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)** angle between two vectors (aligned meaning)

						Once you master this you can create: [search engines](https://medium.com/@deangelaneves/how-to-build-a-search-engine-from-scratch-in-python-part-1-96eb240f9ecb), all kinds of classifications such as sentiment/[spam](https://narengowda.github.io/logistic-regression-sms-spam-ham-classification-using-tfidf-vectorizer/)/sarcasm, or a [Q&A chat bot](https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e) [with large corpus of questions with answers](http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/).

					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 3. condense the data into **meaning**

						* previous methods did not capture meaning
						* worked with large vectors leading to the **curse of dimensionality**
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 3. condense the data into **meaning**

						Reduce dimensionality or condensing meaning using **topic vectors**, we can now search using these topic vectors instead of TF-IDF. This is called **semantic search**.

						* **[Latent Semantic Analysis (LSA)](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)** is equal to Principal Component Analysis (PCA) but for NLP.
							- tries to spread topics as much as possible (good for spam detection)
						* **[Latent Dirichlet Allocation (LDiA)](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)**
							- tries to obtain topics with strongest meaning resemblance (best for most use cases)

						The library [gensim](https://radimrehurek.com/gensim/) can do this for you.
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 3. condense the data into **meaning**

						**Word vectors** or **word embedding**: In effect, a word vector determines the meaning of a word by the type of words it is seen with in proximity.

						Adding the word vectors of a sentence generates a vector that conveys the combined meaning of that sentence: math with words!

						* “king” + “woman” - "man" → “queen”
						* “Harry Potter” + “university” → “Hogwarts”

						Three popular (pre-trained) implementations are Google **[Word2Vec](https://code.google.com/archive/p/word2vec/)**, Stanford's **[GloVe](https://nlp.stanford.edu/projects/glove/)**, and Facebook's **[fastText](https://fasttext.cc)**.

						If you want to [train your own](https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92), train a **skip-gram** or **Continuous Bag of Words (CBOW)** algorithm; for documents use [Doc2vec](https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e).

						Play around with [GloVe](https://lamyiowce.github.io/word2viz/).
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 3. condense the data into **meaning**

						By looking at words independently we have lost information about:
						- the order of the words in a text,
						- the proximity of words within a sentence
						- the hidden layer of meaning within word combination

						Hence:
						**Deep Learning:** from meaning to contextual understanding and generation [using for instance Keras](https://nlpforhackers.io/keras-intro/)

					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## 3. condense the data into **meaning**

						Use **[1D Convolutional Neural Networks](https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/)** (CNN) to crack the more difficult NLP problems:
						- input is a 2D list of word vectors (for each word in a segment) zero-padded to get a fixed size
						- convolution is only 1D, size of the filter window determines considering length of n-grams
						- all other tricks for CNNs apply (pooling, drop out, etc)

						**Cons:** you can only classify text

						<!-- [Demo Sandbox](https://colab.research.google.com/drive/1z3vzQGts7yAAX9tngMRU5nJbdEVJVg51#scrollTo=zZjZ_5l0OTAS&forceEdit=true&sandboxMode=true) -->

					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Towards Advanced Text Generation

<div style="font-size:0.7em;">

| name             | parameters         | release date | by                                      | links                                                                                 | data                                                       |
| ---------------- | ------------------ | ------------ | --------------------------------------- | ------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| GPT-3 (base)     | 2.7B/6.7B/13B/175B | 2020-06-11   | [Open AI](https://openai.com/)          | [wiki](https://en.wikipedia.org/wiki/GPT-3) [github](https://github.com/openai/gpt-3) | [Common Crawl](https://en.wikipedia.org/wiki/Common_Crawl) |
| GPT-Neo          | 2.7B/1.3B/125M     | 2021-03-21   | [Eleuther AI](https://www.eleuther.ai/) | [github](https://github.com/EleutherAI/gpt-neo/)                                      | [The Pile](https://pile.eleuther.ai/)                      |
| GPT-J            | 6B                 | 2021-06-08   | [Eleuther AI](https://www.eleuther.ai/) | [github](https://github.com/kingoflolz/mesh-transformer-jax/)                         | [The Pile](https://pile.eleuther.ai/)                      |
| FLAN             | 137B               | 2021-10-06   | Google                                  | [blog](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html)    |                                                            |
| GPT-3 (instruct) | 2.7B/6.7B/13B/175B | 2022-01-20   | [Open AI](https://openai.com/)          |                                                                                       | [Common Crawl](https://en.wikipedia.org/wiki/Common_Crawl) |
| GPT-NeoX         | 20B                | 2022-02-02   | [Eleuther AI](https://www.eleuther.ai/) | [github](https://github.com/EleutherAI/gpt-neox)                                      | [The Pile](https://pile.eleuther.ai/)                      |
| Jurassic-1       | 7.5B/178B          |              | [AI21 Labs](https://www.ai21.com/)      |                                                                                       |                                                            |
| Fairseq          | 13B                |              | [Meta](https://github.com/pytorch/fairseq/) | [github](https://github.com/pytorch/fairseq/) |                                        |                                                                                       |                                                            |

</div>

						[https://chat.openai.com/](https://chat.openai.com/)
						
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## **Generate** new text
						
						From Vector-to-Vector to Sequence-to-Sequence

						![](https://karpathy.github.io/assets/rnn/diags.jpeg)

					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Seq2Seq Timeline

						- 1986 - Recurrent Neural Networks (RNN)
						- 1997 - Long Short-Term Memory (LSTM)
						- 2017 - Transformers (robots in disguise)

						[TLDR;](https://www.youtube.com/watch?v=TQQlZhbC5ps&ab_channel=CodeEmporium)
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Recurrent Neural Networks (RNN)

						Tries to tackle that A/CNNs expect fixed input and output length, by allowing to rembember previous results, see [explanation](https://victorzhou.com/blog/intro-to-rnns/) and [implementation](https://towardsdatascience.com/implementation-of-rnn-lstm-and-gru-a4250bf6c090).
						
						**Pro:** simple, understandable implementation, useful for simple autocomplete

						**Cons:** exploding/vanishing gradients makes it impractical for larger problems, training is sequential and thus slow
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Long-Short-Term Memory (LSTM)

						To improve on RNNs, we can use a **[Long Short-Term Memory (LSTM)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)** layer ([more explanation](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)), by adding more elaborate forget/memory gates.

						With an LSTM you also capture the time dimension in the [memory cell](https://livebook.manning.com/book/natural-language-processing-in-action/chapter-9/31) of the LSTM layer: **thought vector**. We call this state the thought vector. It not only summarizes a text, it encodes the train-of-thought and context of a statement.

						**Pros:** has longer 'memory' than RNN

						**Cons:** still slow (sequential input). You have little control on the content. Need lots of data for correct grammar.
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## **Generate** new text with the Encoder-Decoder architecture (and LSTMs)

						To improve reuse and text generation we split the problem into an [encoder and decoder](https://livebook.manning.com/book/natural-language-processing-in-action/chapter-10/22) ([more explanation](https://medium.com/analytics-vidhya/intuitive-understanding-of-seq2seq-model-attention-mechanism-in-deep-learning-1c1c24aace1e)).

						Applications:
						- [machine translations](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)
						- Chatbot conversations, Q&A bots, Document summarization, Produce new works of Shakespeare
						- Co-authoring books ([Ronald Giphart's _Ik, Robot_](https://www.vice.com/nl/article/bj7ve4/we-spraken-ronald-giphart-over-hoe-hij-zijn-nieuwe-werk-samen-met-ai-maakte))
						- [Reproducing Trump tweets](https://github.com/eleurent/make-lstm-great-again)

						**Cons:** the thought vector is still very limited, so that e.g. grammar is not properly picked up and relations between words between sentences
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Transformers

						Since the introduction of the Bi-directional [Transformer architecture](https://jalammar.github.io/illustrated-transformer/)[2](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04). NLP has taken another leap. Meaning is no longer context-free as with word embeddings but completely depended on context using **attention**, notable examples: BERT, GPT-2/3, XLNET, T5.
						
						Transformer based models use _self-attention_ and _positional encoding_, _segment encoding_, _multi-headed attention_... 
						
						**Pros:** allows transfer learning, limit has not been reached yet, applicable to other domains

						- [Explanation of BERT](https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/)
						- [BERT Demo](http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)
						- [BERT visualisations](https://github.com/jessevig/bertviz)
						- [Illustrated GPT-2 explanation](https://jalammar.github.io/illustrated-gpt2/)
						- [Transfer learning with Transformers](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Beyond the model

						- How to prevent text-loops, run-off sentences, bad grammar? With [beam-search](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24), [visualized](https://seq2seq-vis.io/).
						- How to validate your output? [Bleu Score / Word Error Metric](https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b)
						
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## What is the full potential of models such as GPT3+?

						With proper prompt engineering 1 model can do:
						- summarization
						- entitiy extraction
						- sentiment analysis
						- Q&A
						- act like a chatbot
						- write and improve computer code
						- basically anything question you can put into words, it will try to do...
						
						Unlock the full potential in the [Advanced workshop](advanced-nlp.html).
					</script>
				</section>
			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true }
				]
			});
		</script>
	</body>
</html>
